{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy.linalg import cholesky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio) # 增幅数据再采样\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userid    M/A     positive_ids #TAB# negative_ids #TAB# date_time #N# positive_ids #TAB# negative_ids #TAB# date_time     [test]positive...#N# negative...\n",
    "\n",
    "def preprocess_user_file(file='ClickData_sample.tsv',npratio=4):  # npratio: K+1 中的 K=4\n",
    "    userid_dict={}\n",
    "    with open(file) as f:\n",
    "        userdata=f.readlines()\n",
    "    for user in userdata:\n",
    "        line=user.strip().split('\\t')\n",
    "        userid=line[0]\n",
    "        if userid not in userid_dict:\n",
    "            userid_dict[userid]=len(userid_dict)\n",
    "    \n",
    "    all_train_id=[]\n",
    "    all_train_pn=[]    \n",
    "    all_label=[]\n",
    "    \n",
    "    all_test_id=[]\n",
    "    all_test_pn=[]    \n",
    "    all_test_label=[]\n",
    "    all_test_index=[]\n",
    "    \n",
    "    all_user_pos=[]\n",
    "    all_test_user_pos=[]\n",
    "    \n",
    "    for user in userdata:\n",
    "        line=user.strip().split('\\t')\n",
    "        userid=line[0]\n",
    "        if len(line)==4:\n",
    "            \n",
    "            impre=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "        if len(line)==3:\n",
    "            impre=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "    \n",
    "        trainpos=[x[0].split() for x in impre]  # positive\n",
    "        trainneg=[x[1].split() for x in impre]  # negtive\n",
    "\n",
    "        # train_datetime = [x[2].split() for x in impre] \n",
    "        \n",
    "        poslist=list(itertools.chain(*(trainpos)))\n",
    "        neglist=list(itertools.chain(*(trainneg)))\n",
    "\n",
    "        # 那么对应关系呢\n",
    "    \n",
    "        \n",
    "        if len(line)==4:    # ok, 最后是测试数据\n",
    "            testimpre=[x.split('#TAB#') for x in line[3].split('#N#')]\n",
    "            testpos=[x[0].split() for x in testimpre]\n",
    "            testneg=[x[1].split() for x in testimpre]\n",
    "            \n",
    "            \n",
    "            for i in range(len(testpos)):\n",
    "                sess_index=[]\n",
    "                sess_index.append(len(all_test_pn))\n",
    "                posset=list(set(poslist))   # poslist没有更新，这句没必要放在for 里面。。\n",
    "                allpos=[int(p) for p in random.sample(posset,min(50,len(posset)))[:50]] # 最多随机采样50个正样本\n",
    "                allpos+=[0]*(50-len(allpos))    # 后面补零\n",
    "        \n",
    "                \n",
    "                for j in testpos[i]:\n",
    "                    all_test_pn.append(int(j))\n",
    "                    all_test_label.append(1)\n",
    "                    all_test_id.append(userid_dict[userid]) # 反复append？？？\n",
    "                    all_test_user_pos.append(allpos)\n",
    "                    \n",
    "                for j in testneg[i]:\n",
    "                    all_test_pn.append(int(j))\n",
    "                    all_test_label.append(0)\n",
    "                    all_test_id.append(userid_dict[userid])\n",
    "                    all_test_user_pos.append(allpos)\n",
    "                sess_index.append(len(all_test_pn))\n",
    "                all_test_index.append(sess_index)\n",
    "                \n",
    "    \n",
    "    \n",
    "                \n",
    "        for impre_id in range(len(trainpos)):\n",
    "            for pos_sample in trainpos[impre_id]:\n",
    "    \n",
    "                pos_neg_sample=newsample(trainneg[impre_id],npratio)\n",
    "                pos_neg_sample.append(pos_sample)\n",
    "                temp_label=[0]*npratio+[1]\n",
    "                temp_id=list(range(npratio+1))\n",
    "                random.shuffle(temp_id)\n",
    "    \n",
    "                \n",
    "                shuffle_sample=[]\n",
    "                shuffle_label=[]\n",
    "                for id in temp_id:\n",
    "                    shuffle_sample.append(int(pos_neg_sample[id]))\n",
    "                    shuffle_label.append(temp_label[id])\n",
    "                \n",
    "                posset=list(set(poslist)-set([pos_sample]))\n",
    "                allpos=[int(p) for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "                allpos+=[0]*(50-len(allpos))\n",
    "                all_train_pn.append(shuffle_sample)\n",
    "                all_label.append(shuffle_label)\n",
    "                all_train_id.append(userid_dict[userid])\n",
    "                all_user_pos.append(allpos)\n",
    "            \n",
    "    all_train_pn=np.array(all_train_pn,dtype='int32')\n",
    "    all_label=np.array(all_label,dtype='int32')\n",
    "    all_train_id=np.array(all_train_id,dtype='int32')\n",
    "    all_test_pn=np.array(all_test_pn,dtype='int32')\n",
    "    all_test_label=np.array(all_test_label,dtype='int32')\n",
    "    all_test_id=np.array(all_test_id,dtype='int32')\n",
    "    all_user_pos=np.array(all_user_pos,dtype='int32')\n",
    "    all_test_user_pos=np.array(all_test_user_pos,dtype='int32')\n",
    "    return userid_dict,all_train_pn,all_label,all_train_id,all_test_pn,all_test_label,all_test_id,all_user_pos,all_test_user_pos,all_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news_file(file='DocMeta_sample.tsv'):\n",
    "    with open(file) as f:\n",
    "        newsdata=f.readlines()\n",
    "    \n",
    "    news={}\n",
    "    for newsline in newsdata:\n",
    "        line=newsline.strip().split('\\t')\n",
    "        news[line[1]]=[line[2],line[3],word_tokenize(line[6].lower())]  # map<id, [cate, category, tokenized_title]>\n",
    "    word_dict_raw={'PADDING':[0,999999]}\n",
    "    \n",
    "    for docid in news:\n",
    "        for word in news[docid][2]:\n",
    "            if word in word_dict_raw:\n",
    "                word_dict_raw[word][1]+=1\n",
    "            else:\n",
    "                word_dict_raw[word]=[len(word_dict_raw),1]  # map<tokenized_title_word(index), [current_map_size, word_count]>\n",
    "    word_dict={}\n",
    "    for i in word_dict_raw:\n",
    "        if word_dict_raw[i][1]>=2:\n",
    "            word_dict[i]=[len(word_dict),word_dict_raw[i][1]]   # map<tokenized_title_word, [current_map_size(index), word_count]> & word_count >= 2\n",
    "    print(len(word_dict),len(word_dict_raw))\n",
    "    \n",
    "    news_words=[[0]*30]\n",
    "    news_index={'0':0}\n",
    "    for newsid in news:\n",
    "        word_id=[]\n",
    "        news_index[newsid]=len(news_index)  # map<news_id, index(0,1,2...)> ，写法好怪异！！！为什么不直接用enumerate()\n",
    "        for word in news[newsid][2]:\n",
    "            if word in word_dict:\n",
    "                word_id.append(word_dict[word][0])  # all indices\n",
    "        word_id=word_id[:30]\n",
    "        news_words.append(word_id+[0]*(30-len(word_id)))\n",
    "    news_words=np.array(news_words,dtype='int32') \n",
    "    return word_dict,news_words,news_index  # news_words & news_index all stored indices, news_words:dim3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_dict):\n",
    "    embedding_dict={}\n",
    "    cnt=0\n",
    "    with open('/raid/glove.840B.300d.txt','rb')as f:   # line0: word_id, line1,2,3...: word_embedding\n",
    "        linenb=0\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if len(line)==0:\n",
    "                break\n",
    "            line = line.split()\n",
    "            word=line[0].decode()\n",
    "            linenb+=1\n",
    "            if len(word) != 0:\n",
    "                vec=[float(x) for x in line[1:]]\n",
    "                if word in word_dict:\n",
    "                    embedding_dict[word]=vec\n",
    "                    if cnt%1000==0:\n",
    "                        print(cnt,linenb,word)\n",
    "                    cnt+=1\n",
    "\n",
    "    embedding_matrix=[0]*len(word_dict) # 这个初始化意义不明，哦懂了，就是可以直接用index给array赋值，不会出outofindex错误\n",
    "    cand=[] # 这是个什么玩意儿\n",
    "    for i in embedding_dict:\n",
    "        embedding_matrix[word_dict[i][0]]=np.array(embedding_dict[i],dtype='float32')   # index->word_embedding，就是这里的直接赋值\n",
    "        cand.append(embedding_matrix[word_dict[i][0]])  # cand without word_dict's index\n",
    "    cand=np.array(cand,dtype='float32') # cand is the matrix without order\n",
    "    mu=np.mean(cand, axis=0)\n",
    "    Sigma=np.cov(cand.T)\n",
    "    norm=np.random.multivariate_normal(mu, Sigma, 1)    # random multivariate distribution\n",
    "    for i in range(len(embedding_matrix)):\n",
    "        if type(embedding_matrix[i])==int:  # 好家伙，用这个判断是不是空。。。\n",
    "            embedding_matrix[i]=np.reshape(norm, 300)   # 如果是空就赋值随机分布数据\n",
    "    embedding_matrix[0]=np.zeros(300,dtype='float32')\n",
    "    embedding_matrix=np.array(embedding_matrix,dtype='float32')\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict,all_train_pn,all_label,all_train_id,all_test_pn,all_test_label,all_test_id,all_user_pos,all_test_user_pos,all_test_index=preprocess_user_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "56 388\n"
     ]
    }
   ],
   "source": [
    "word_dict,news_words,news_index=preprocess_news_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 1 ,\n",
      "(56, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_mat=get_embedding(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # np.arange(x) == np.array(range(x))\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_train(all_train_pn,all_label,all_train_id,batch_size):\n",
    "    inputid = np.arange(len(all_label))\n",
    "    np.random.shuffle(inputid)\n",
    "    y=all_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            candidate = news_words[all_train_pn[i]]\n",
    "            candidate_split=[candidate[:,k,:] for k in range(candidate.shape[1])]   # candidate[:,k,:] ????\n",
    "            browsed_news=news_words[all_user_pos[i]]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            label=all_label[i]\n",
    "            yield (candidate_split +browsed_news_split+[userid], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_test(all_test_pn,all_label,all_test_id,batch_size):\n",
    "    inputid = np.arange(len(all_label))\n",
    "    y=all_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            candidate = news_words[all_test_pn[i]]\n",
    "            browsed_news=news_words[all_test_user_pos[i]]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_test_id[i],axis=1)\n",
    "            label=all_label[i]\n",
    "\n",
    "            yield ([candidate]+ browsed_news_split+[userid], label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1267847ef1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import keras\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, Input, Convolution1D, Dot, Activation\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import *\n",
    "npratio=4\n",
    "results=[]\n",
    "    \n",
    "    \n",
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "user_id = Input(shape=(1,), dtype='int32')\n",
    "user_embedding_layer= Embedding(len(userid_dict), 50,trainable=True)    # 变化的\n",
    "user_embedding= user_embedding_layer(user_id)\n",
    "user_embedding_word= Dense(200,activation='relu')(user_embedding)\n",
    "user_embedding_word= Flatten()(user_embedding_word) \n",
    "user_embedding_news= Dense(200,activation='relu')(user_embedding)\n",
    "user_embedding_news= Flatten()(user_embedding_news)\n",
    "\n",
    "\n",
    "news_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict) , 300, weights=[embedding_mat],trainable=True)\n",
    "embedded_sequences = embedding_layer(news_input)\n",
    "embedded_sequences =Dropout(0.2)(embedded_sequences)\n",
    "\n",
    "cnnouput = Convolution1D(filters=400, kernel_size=3,  padding='same', activation='relu', strides=1)(embedded_sequences )\n",
    "cnnouput=Dropout(0.2)(cnnouput)\n",
    "\n",
    "\n",
    "attention_a = Dot((2, 1))([cnnouput, Dense(400,activation='tanh')(user_embedding_word)])\n",
    "attention_weight = Activation('softmax')(attention_a)\n",
    "news_rep=keras.layers.Dot((1, 1))([cnnouput, attention_weight])\n",
    "newsEncoder = Model([news_input,user_id], news_rep)\n",
    "\n",
    "\n",
    "all_news_input = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "browsed_news_rep = [newsEncoder([news,user_id]) for news in all_news_input]\n",
    "browsed_news_rep =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(news) for news in browsed_news_rep],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "attention_news = keras.layers.Dot((2, 1))([browsed_news_rep, Dense(400,activation='tanh')(user_embedding_news)])\n",
    "attention_weight_news = Activation('softmax')(attention_news)\n",
    "user_rep=keras.layers.Dot((1, 1))([browsed_news_rep, attention_weight_news])\n",
    "\n",
    "\n",
    "candidates = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "candidate_vecs = [ newsEncoder([candidate,user_id]) for candidate in candidates]\n",
    "logits = [keras.layers.dot([user_rep, candidate_vec], axes=-1) for candidate_vec in candidate_vecs]\n",
    "logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "\n",
    "model = Model(candidates+all_news_input+[user_id], logits)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = newsEncoder([candidate_one,user_id])\n",
    "score = keras.layers.Activation(keras.activations.sigmoid)(keras.layers.dot([user_rep, candidate_one_vec], axes=-1))\n",
    "model_test = keras.Model([candidate_one]+all_news_input+[user_id], score)\n",
    "\n",
    "for ep in range(3):\n",
    "    traingen=generate_batch_data_train(all_train_pn,all_label,all_train_id, 100)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(all_train_id)//100)\n",
    "    testgen=generate_batch_data_test(all_test_pn,all_test_label,all_test_id, 100)\n",
    "    click_score = model_test.predict_generator(testgen, steps=len(all_test_id)//100,verbose=1)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for m in all_test_index:\n",
    "        if np.sum(all_test_label[m[0]:m[1]])!=0 and m[1]<len(click_score):\n",
    "            all_auc.append(roc_auc_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0]))\n",
    "            all_mrr.append(mrr_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0]))\n",
    "            all_ndcg.append(ndcg_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0],k=5))\n",
    "            all_ndcg2.append(ndcg_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0],k=10))\n",
    "    results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0ab056dc25f0889fcb574ae6e472eade94d42e26aa4302f891626f1c166c36ce7",
   "display_name": "Python 3.8.5 64-bit ('dev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}